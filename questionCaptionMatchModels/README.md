# questionCaptionMatchModels

Code for identifying True vs False Premise questions for a given image as described in the paper, Question Relevance in VQA:
Identifying Non-Visual And False-Premise Questions,  https://arxiv.org/pdf/1606.06622v2.pdf

### Pre-requisites:

- Spacy: 
  - ```pip install spacy```
  - ```sputnik --name spacy --repository-url http://index.spacy.io install en==1.1.0```

- Keras Deep Learning Library:
  - ```pip install keras```

- Scikit Learn Package
  - ```pip install -U scikit-learn```


### How to run the code:

```python questionCaptionModel.py --captype [option1] --model [option2] --loadweights [option3] --saveModel [option4]```

Options are:

- [option1] :
  - ```qc``` for question-caption similarity.
  - ```qq``` for question-question similarity.
  - ```qdq``` This method is not described in the paper and is an experimental method where the similarity is computed between the test question and a set of diverse questions generated by a captioning model. 
  
- [option2] :
  - ```bow``` : concatenates test question and generated question/caption features by Bag-of-Words technique 
  - ```avgw2v``` : concatenates test question and generated question/caption features by averaging word2vec.
  - ```lstm``` : concatenates test question and generated question/caption features by feeding in the word2vec vectors into an lstm
  
- [option3] : (optional) Path of pretrained weights. You will find some pretrained weights in the ```outputmodels/``` folder. The following are the weights used in the final paper:
  - Question-Caption Match:
    - BoW:  ```outputmodels/2016-08-24\ 02\:51\:48.482004_qc_bow.h5```
    - Avg Word2Vec:  ```outputmodels/2016-08-24\ 02\:47\:17.626481_qc_avgw2v.h5```
    - LSTM:  ```outputmodels/2016-08-24\ 03\:32\:01.409005_qc_lstm.h5 ```
  - Question-Question Match:
    - BoW:   ```outputmodels/2016-08-24\ 02\:29\:46.027887_qq_bow.h5```
    - Avg Word2Vec: ```outputmodels/2016-08-24\ 02\:19\:07.961071_qq_avgw2v.h5```
    - LSTM:  ```outputmodels/2016-08-24\ 11\:54\:29.343364_qq_lstm.h5```

- [option4] : If [option3] is not specified, the training is executed. Set ```FALSE``` to NOT save your trained model at the end of execution, default value is ```TRUE``` (i.e trained model will be saved)

NOTE: The Questions/Captions generated for the images in the Visual True vs False Premise data have already been pre generated here. If you want to run it on your own set of images and questions, you have to generate the captions/questions for the images using a caption generator. I used Andrej Karpathy's NeuralTalk2. Code and instructions for generating captions in the format our models need will be uploaded soon. Please email me if you need them immediately.  

Please contact ray93@vt.edu if you have any questions.
